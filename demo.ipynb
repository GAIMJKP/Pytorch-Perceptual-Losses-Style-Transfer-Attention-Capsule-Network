{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TZN_cDShpIYn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels\n",
        "!pip install git+https://github.com/leftthomas/CapsuleLayer.git@master\n",
        "!pip install git+https://github.com/szagoruyko/pytorchviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PYeea4mjpTmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 挂载谷歌云盘\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kxMcIiM_pYdD",
        "colab_type": "code",
        "outputId": "9e997469-a1e8-47ce-9550-dc63d17fe81b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# 更改云工作目录\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "import os\n",
        "os.chdir(\"drive/AI/styleTransfer/pytorch_perceptual\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuse: mountpoint is not empty\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0qLMHZ53Rewg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch.optim import Optimizer\n",
        "import torchvision\n",
        "import torchvision.transforms as Transforms\n",
        "import torchvision.datasets as Datasets\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import datetime\n",
        "import math\n",
        "import pretrainedmodels\n",
        "from capsule_layer import CapsuleConv2d\n",
        "from capsule_layer import CapsuleConvTranspose2d\n",
        "from capsule_layer.optim import MultiStepRI\n",
        "\n",
        "# Set random number seed\n",
        "SEED = 66\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NMkNUnPWTQcE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# \"Fixing Weight Decay Regularization in Adam\" https://arxiv.org/abs/1711.05101\n",
        "class AdamW(Optimizer):\n",
        "    \"\"\"Implements Adam algorithm.\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            ICLR 2018 https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, amsgrad=False):\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "        #super(AdamW, self).__init__(params, defaults)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                if amsgrad:\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                else:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    decayed_weights = torch.mul(p.data, group['weight_decay'])\n",
        "                    p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "                    p.data.sub_(decayed_weights)\n",
        "                else:\n",
        "                    p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, stride=1, norm_type='instance', norm_first=True, norm_last=False):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.stride = stride\n",
        "        self.norm_first = norm_first\n",
        "        self.norm_last = norm_last\n",
        "        \n",
        "        if norm_type is 'batch':\n",
        "            self.norm = nn.BatchNorm2d\n",
        "        else:\n",
        "            self.norm = nn.InstanceNorm2d\n",
        "        \n",
        "        if self.norm_first:\n",
        "            self.bn_first = self.norm(input_channels, affine=True)\n",
        "            self.relu_first = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(input_channels, int(output_channels/4), 1, 1, bias = False)\n",
        "        self.bn1 = self.norm(int(output_channels/4), affine=True)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(int(output_channels/4), int(output_channels/4), 3, stride, padding = 1, bias = False)\n",
        "        self.bn2 = self.norm(int(output_channels/4), affine=True)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv3 = nn.Conv2d(int(output_channels/4), output_channels, 1, 1, bias = False)\n",
        "        if (self.input_channels != self.output_channels) or (self.stride !=1 ):\n",
        "            self.conv4 = nn.Conv2d(input_channels, output_channels , 1, stride, bias = False)\n",
        "        if self.norm_last:\n",
        "            self.bn_last = self.norm(input_channels, affine=True)\n",
        "            self.relu_last = nn.ReLU(inplace=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.norm_first:\n",
        "            out = self.bn_first(x)\n",
        "            out1 = self.relu_first(out)\n",
        "        else:\n",
        "            out1 = residual\n",
        "        out = self.conv1(out1)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.conv3(out)\n",
        "        if (self.input_channels != self.output_channels) or (self.stride !=1 ):\n",
        "            residual = self.conv4(out1)\n",
        "        out += residual\n",
        "        if self.norm_last:\n",
        "            out = self.bn_last(out)\n",
        "            out = self.relu_last(out)\n",
        "        return out\n",
        "      \n",
        "# \"Residual Attention Network for Image Classification\" https://arxiv.org/pdf/1704.06904.pdf\n",
        "class AttentionModule_stage1(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, norm_type='instance'):\n",
        "        super(AttentionModule_stage1, self).__init__()\n",
        "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels, norm_type='instance', norm_first=False)\n",
        "\n",
        "        self.trunk_branches = nn.Sequential(\n",
        "            ResidualBlock(in_channels, out_channels, norm_type='instance'),\n",
        "            ResidualBlock(in_channels, out_channels, norm_type='instance')\n",
        "         )\n",
        "\n",
        "        if norm_type is 'batch':\n",
        "            self.norm = nn.BatchNorm2d\n",
        "        else:\n",
        "            self.norm = nn.InstanceNorm2d\n",
        "        \n",
        "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.softmax1_blocks = ResidualBlock(in_channels, out_channels, norm_type='instance')\n",
        "        self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels, norm_type='instance')\n",
        "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.softmax2_blocks = ResidualBlock(in_channels, out_channels, norm_type='instance')\n",
        "        self.skip2_connection_residual_block = ResidualBlock(in_channels, out_channels, norm_type='instance')\n",
        "        self.mpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.softmax3_blocks = nn.Sequential(\n",
        "            ResidualBlock(in_channels, out_channels, norm_type='instance'),\n",
        "            ResidualBlock(in_channels, out_channels, norm_type='instance')\n",
        "        )\n",
        "\n",
        "        self.softmax4_blocks = ResidualBlock(in_channels, out_channels, norm_type='instance')\n",
        "\n",
        "        self.softmax5_blocks = ResidualBlock(in_channels, out_channels, norm_type='instance')\n",
        "\n",
        "        self.softmax6_blocks = nn.Sequential(\n",
        "            self.norm(out_channels, affine=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels , kernel_size = 1, stride = 1, bias = False),\n",
        "            self.norm(out_channels, affine=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels , kernel_size = 1, stride = 1, bias = False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.last_blocks = ResidualBlock(in_channels, out_channels, norm_type='instance', norm_last=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first_residual_blocks(x)\n",
        "        out_trunk = self.trunk_branches(x)\n",
        "        out_mpool1 = self.mpool1(x)\n",
        "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
        "        out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
        "        out_mpool2 = self.mpool2(out_softmax1)\n",
        "        out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
        "        out_skip2_connection = self.skip2_connection_residual_block(out_softmax2)\n",
        "        out_mpool3 = self.mpool3(out_softmax2)\n",
        "        out_softmax3 = self.softmax3_blocks(out_mpool3)\n",
        "        out_interp3 = F.interpolate(out_softmax3, (out_softmax2.shape[2], out_softmax2.shape[3])) + out_softmax2\n",
        "        out = out_interp3 + out_skip2_connection\n",
        "        out_softmax4 = self.softmax4_blocks(out)\n",
        "        out_interp2 = F.interpolate(out_softmax4, (out_softmax1.shape[2], out_softmax1.shape[3])) + out_softmax1\n",
        "        out = out_interp2 + out_skip1_connection\n",
        "        out_softmax5 = self.softmax5_blocks(out)\n",
        "        out_interp1 = F.interpolate(out_softmax5, (out_trunk.shape[2], out_trunk.shape[3])) + out_trunk\n",
        "        out_softmax6 = self.softmax6_blocks(out_interp1)\n",
        "        out = (1 + out_softmax6) * out_trunk\n",
        "        out_last = self.last_blocks(out)\n",
        "\n",
        "        return out_last\n",
        "\n",
        "# \"Squeeze-and-Excitation Networks\" https://arxiv.org/abs/1709.01507\n",
        "class SE_Resnet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SE_Resnet50, self).__init__()\n",
        "        print('Preparing pretrained SE Resnet 50 ...')\n",
        "        self.se_resnet50 = pretrainedmodels.__dict__['se_resnet50'](num_classes=1000, pretrained='imagenet').eval()\n",
        "        self.layer0 = self.se_resnet50.layer0\n",
        "        self.layer1_1 = self.se_resnet50.layer1[0]\n",
        "        self.layer1_2 = self.se_resnet50.layer1[1]\n",
        "        self.layer1_3 = self.se_resnet50.layer1[2]\n",
        "        self.layer2_1 = self.se_resnet50.layer2[0]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layer0(x)\n",
        "        out_1 = self.layer1_1(x)\n",
        "        out_2 = self.layer1_2(out_1)\n",
        "        out_3 = self.layer1_3(out_2)\n",
        "        out_4 = self.layer2_1(out_3)\n",
        "\n",
        "        return [out_1, out_2, out_3, out_4]\n",
        "\n",
        "# Load image with size of parameter size\n",
        "def load_img(path, scale=None, size=None):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "\n",
        "    transform_list = []\n",
        "    if size is not None:\n",
        "        transform_list += [Transforms.Resize((int(size[0]), int(size[1])))]\n",
        "    elif scale is not None:\n",
        "        transform_list += [Transforms.Resize((int(img.size[1] * scale), int(img.size[0] * scale)))]\n",
        "\n",
        "    transform_list += [Transforms.ToTensor()]\n",
        "    transform = Transforms.Compose(transform_list)\n",
        "    \n",
        "    img = transform(img)\n",
        "    img = img.unsqueeze(dim=0)\n",
        "    \n",
        "    return img\n",
        "\n",
        "# Make image with shape of [content | result | style] and save it\n",
        "def save_img(img_name, content, style, result):\n",
        "    _, H, W = content.size()\n",
        "    size = (H, W)\n",
        "    # print(content.shape,result.shape,style.shape)\n",
        "    img = torch.stack([content, result, style], dim=0)\n",
        "    torchvision.utils.save_image(img, img_name, nrow=3)\n",
        "\n",
        "# Load pretrained weight\n",
        "def load_weight(model, path):\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    return model\n",
        "\n",
        "# Normalization with mean and std\n",
        "def norm(var):\n",
        "    if torch.cuda.is_available():\n",
        "        dtype = torch.cuda.FloatTensor\n",
        "    else:\n",
        "        dtype = torch.FloatTensor\n",
        "    \n",
        "    mean = Variable(torch.zeros(var.size()).type(dtype))\n",
        "    std = Variable(torch.zeros(var.size()).type(dtype))\n",
        "    \n",
        "    mean[:, 0, :, :] = 0.485\n",
        "    mean[:, 1, :, :] = 0.456\n",
        "    mean[:, 2, :, :] = 0.406\n",
        "    \n",
        "    std[:, 0, :, :] = 0.229\n",
        "    std[:, 1, :, :] = 0.224\n",
        "    std[:, 2, :, :] = 0.225\n",
        "\n",
        "    normed = var.sub(mean).div(std)\n",
        "    return normed\n",
        "\n",
        "# Denormalization with mean and std\n",
        "def denorm(var):\n",
        "    if torch.cuda.is_available():\n",
        "        dtype = torch.cuda.FloatTensor\n",
        "    else:\n",
        "        dtype = torch.FloatTensor\n",
        "\n",
        "    mean = Variable(torch.zeros(var.size()).type(dtype))\n",
        "    std = Variable(torch.zeros(var.size()).type(dtype))\n",
        "    \n",
        "    mean[:, 0, :, :] = 0.485\n",
        "    mean[:, 1, :, :] = 0.456\n",
        "    mean[:, 2, :, :] = 0.406\n",
        "    \n",
        "    std[:, 0, :, :] = 0.229\n",
        "    std[:, 1, :, :] = 0.224\n",
        "    std[:, 2, :, :] = 0.225\n",
        "\n",
        "    normed = var.mul(std).add(mean)\n",
        "    return normed\n",
        "\n",
        "# Get gram matrix\n",
        "def gram(var_list):\n",
        "    gram_list = []\n",
        "    \n",
        "    for i in range(len(var_list)):\n",
        "        var = var_list[i]\n",
        "        N, C, H, W = var.size()\n",
        "        var = var.view(N, C, H*W)\n",
        "        g = torch.bmm(var, var.transpose(2, 1)) / (C * H * W)\n",
        "        gram_list.append(g)\n",
        "        \n",
        "    return gram_list\n",
        "\n",
        "# Get doubleGram matrix\n",
        "def firstGram(tensor1,tensor2):\n",
        "    g=[]\n",
        "    for i in range(tensor1.shape[0]):\n",
        "        g.append((tensor2[i]*tensor1))\n",
        "    return torch.cat(g,0)\n",
        "\n",
        "def secondGram(tensor1):\n",
        "    g=[]\n",
        "    for i in range(tensor1.shape[0]):\n",
        "        for j in range(tensor1.shape[0]):\n",
        "            g.append(firstGram(tensor1[i],tensor1[j]))\n",
        "    return torch.cat(g,0)\n",
        "\n",
        "def doubleGram(tensor1):\n",
        "    g=[]\n",
        "    for i in range(tensor1.shape[0]):\n",
        "        g.append(secondGram(tensor1[i]).unsqueeze(0))\n",
        "    return torch.cat(g,0)\n",
        "\n",
        "def gramP(var_list):\n",
        "    gram_list = []\n",
        "    \n",
        "    for i in range(len(var_list)):\n",
        "        var = var_list[i]\n",
        "        N, C, H, W = var.size()\n",
        "        var = var.view(N, C, H*W)\n",
        "        g = doubleGram(var) / (C * H * W * H * W)\n",
        "        gram_list.append(g)\n",
        "        \n",
        "    return gram_list\n",
        "\n",
        "# Creat data loader\n",
        "def data_loader(root, batch_size=4, size=256):\n",
        "    transform = Transforms.Compose([Transforms.Resize(size), \n",
        "                                    Transforms.CenterCrop(size),\n",
        "                                    Transforms.ToTensor()\n",
        "                                   ])\n",
        "    dataset = Datasets.ImageFolder(root, transform)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    length = len(dataset)\n",
        "    \n",
        "    return loader, length\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, kernel=3, stride=2, pad=1, norm_type='batch', act_type='relu'):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        layers = []\n",
        "        layers += [nn.ReflectionPad2d(pad), \n",
        "                   nn.Conv2d(in_c, out_c, kernel_size=kernel, stride=stride, padding=0)]\n",
        "        \n",
        "        if norm_type is 'batch':\n",
        "            layers += [nn.BatchNorm2d(out_c, affine=True)]\n",
        "        elif norm_type is 'instance':\n",
        "            layers += [nn.InstanceNorm2d(out_c, affine=True)]\n",
        "        elif norm_type is None:\n",
        "            pass\n",
        "        \n",
        "        if act_type is 'relu': \n",
        "            layers += [nn.ReLU()]\n",
        "        elif act_type is 'tanh':\n",
        "            layers += [nn.Tanh()]\n",
        "        elif act_type is None:\n",
        "            pass\n",
        "        \n",
        "        self.block = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "    \n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels, norm_type):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.block = nn.Sequential(ConvBlock(channels, channels, kernel=3, stride=1, pad=1, norm_type=norm_type,\n",
        "                                             act_type='relu'),\n",
        "                                   ConvBlock(channels, channels, kernel=3, stride=1, pad=1, norm_type=norm_type,\n",
        "                                             act_type=None))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + x\n",
        "        return out\n",
        "\n",
        "class ConvTransBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, kernel=3, stride=2, pad=1, out_pad=1, norm_type='batch'):\n",
        "        super(ConvTransBlock, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        # Conv transpose layer\n",
        "        layers += [nn.ConvTranspose2d(in_c, out_c, kernel_size=kernel, stride=stride, padding=pad,\n",
        "                                      output_padding=out_pad)]\n",
        "        \n",
        "        # Normalization layer\n",
        "        if norm_type is 'batch':\n",
        "            layers += [nn.BatchNorm2d(out_c, affine=True)]\n",
        "        elif norm_type is 'instance':\n",
        "            layers += [nn.InstanceNorm2d(out_c, affine=True)]\n",
        "\n",
        "        # Activiation layer\n",
        "        layers += [nn.ReLU()]\n",
        "        \n",
        "        self.conv_trans_block = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv_trans_block(x)\n",
        "        return out\n",
        "    \n",
        "class ImageTransformNet(nn.Module):\n",
        "    def __init__(self, res_block_num=5, norm_type='batch'):\n",
        "        super(ImageTransformNet, self).__init__()\n",
        "        # Downsampling blocks\n",
        "        self.downsamples = nn.Sequential(ConvBlock(3, 32, kernel=9, stride=1, pad=4, norm_type=norm_type,\n",
        "                                                   act_type='relu'),\n",
        "                                         ConvBlock(32, 64, kernel=3, stride=2, pad=1, norm_type=norm_type,\n",
        "                                                   act_type='relu'),\n",
        "                                         ConvBlock(64, 128, kernel=3, stride=2, pad=1, norm_type=norm_type,\n",
        "                                                   act_type='relu'))\n",
        "        \n",
        "        # Residual blocks\n",
        "        res = []\n",
        "        for _ in range(res_block_num): \n",
        "            res += [ResBlock(128, norm_type)]\n",
        "        self.residuals = nn.Sequential(*res)    \n",
        "\n",
        "        # Upsampling blocks\n",
        "        self.upsamples = nn.Sequential(ConvTransBlock(128, 64, kernel=3, stride=2, pad=1, out_pad=1,\n",
        "                                                      norm_type=norm_type),\n",
        "                                       ConvTransBlock(64, 32, kernel=3, stride=2, pad=1, out_pad=1,\n",
        "                                                      norm_type=norm_type),\n",
        "                                       ConvBlock(32, 3, kernel=9, stride=1, pad=4, norm_type=None,\n",
        "                                                 act_type='tanh'))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.downsamples(x)\n",
        "        out = self.residuals(out)\n",
        "        out = self.upsamples(out)\n",
        "        out = (out + 1) / 2\n",
        "        return out\n",
        "\n",
        "class CapsBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, in_length, out_length, kernel=3, stride=2, pad=1):\n",
        "        super(CapsBlock, self).__init__()\n",
        "        layers = []\n",
        "        layers += [nn.ReflectionPad2d(pad), \n",
        "                   CapsuleConv2d(in_c, out_c, in_length=in_length, out_length=out_length, kernel_size=kernel, \n",
        "                                 stride=stride, padding=0, routing_type='dynamic', bias=True)]\n",
        "        \n",
        "        self.block = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "    \n",
        "class CapsResBlock(nn.Module):\n",
        "    def __init__(self, channels, vlength=8):\n",
        "        super(CapsResBlock, self).__init__()\n",
        "        self.block = nn.Sequential(CapsBlock(channels, channels, vlength, vlength, kernel=3, stride=1, pad=1),\n",
        "                                   CapsBlock(channels, channels, vlength, vlength, kernel=3, stride=1, pad=1))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + x\n",
        "        return out\n",
        "    \n",
        "class CapsTransBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, in_length, out_length, kernel=3, stride=2, pad=1, out_pad=1):\n",
        "        super(CapsTransBlock, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        # Conv transpose layer\n",
        "        layers += [CapsuleConvTranspose2d(in_c, out_c, in_length=in_length, out_length=out_length, kernel_size=kernel, \n",
        "                                          stride=stride, padding=pad, output_padding=out_pad, bias=True)]\n",
        "        \n",
        "        self.conv_trans_block = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv_trans_block(x)\n",
        "        return out\n",
        "\n",
        "class ImageTransformCapsNet(nn.Module):\n",
        "    def __init__(self, res_block_num=5, norm_type='batch'):\n",
        "        super(ImageTransformCapsNet, self).__init__()\n",
        "        # Downsampling blocks\n",
        "        self.downsamples = nn.Sequential(ConvBlock(3, 32, kernel=9, stride=1, pad=4, norm_type=norm_type,\n",
        "                                                   act_type='relu'),\n",
        "                                         ConvBlock(32, 64, kernel=3, stride=2, pad=1, norm_type=norm_type,\n",
        "                                                   act_type='relu'),\n",
        "                                         ConvBlock(64, 128, kernel=3, stride=2, pad=1, norm_type=norm_type,\n",
        "                                                   act_type='relu'))\n",
        "        \n",
        "        # Residual attention blocks\n",
        "        res = []\n",
        "        for _ in range(res_block_num): \n",
        "            res += [ResBlock(128, norm_type)]\n",
        "        self.residuals = nn.Sequential(*res)    \n",
        "        \n",
        "        self.attention = AttentionModule_stage1(128, 128, norm_type=norm_type)\n",
        "    \n",
        "        # Upsampling blocks\n",
        "        self.upsamples = nn.Sequential(CapsTransBlock(128, 64, 64, 16, kernel=3, stride=2, pad=1, out_pad=1),\n",
        "                                       CapsTransBlock(64, 32, 16, 32, kernel=3, stride=2, pad=1, out_pad=1,),\n",
        "                                       ConvBlock(32, 3, kernel=9, stride=1, pad=4, norm_type=None, act_type='tanh'),\n",
        "                                       # CapsBlock(32, 3, 8, 1, kernel=9, stride=1, pad=4),\n",
        "                                       )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.downsamples(x)\n",
        "        out = self.residuals(out)\n",
        "        out = self.upsamples(out)\n",
        "        out = (out + 1) / 2\n",
        "        return out\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        print('Preparing pretrained VGG 16 ...')\n",
        "        self.vgg_16 = torchvision.models.vgg16(pretrained=True).features\n",
        "        \n",
        "        self.relu_1_2 = nn.Sequential(*list(self.vgg_16.children())[0:4])\n",
        "        self.relu_2_2 = nn.Sequential(*list(self.vgg_16.children())[4:9])\n",
        "        self.relu_3_3 = nn.Sequential(*list(self.vgg_16.children())[9:16])\n",
        "        self.relu_4_3 = nn.Sequential(*list(self.vgg_16.children())[16:23])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out_1_2 = self.relu_1_2(x)\n",
        "        out_2_2 = self.relu_2_2(out_1_2)\n",
        "        out_3_3 = self.relu_3_3(out_2_2)\n",
        "        out_4_3 = self.relu_4_3(out_3_3)\n",
        "\n",
        "        return [out_1_2, out_2_2, out_3_3, out_4_3]\n",
        "    \n",
        "class Solver():\n",
        "    def __init__(self, trn_dir, style_path, result_dir, weight_dir, process_dir=None, process_image=None, process_scale=None, process_number=0, record_number=0, num_epoch=2, \n",
        "                 batch_size=4, record_name=None, content_loss_pos=2, lr=1e-3, lambda_c=1, lambda_s=5e+5, show_every=1000, save_every=5000, pretrain=None, lossNet='vgg',\n",
        "                 test_number=0, test_dir=None, transNet='capsnet', opti='adamw', norm_type='batch', gram='gramP'):\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            self.dtype = torch.cuda.FloatTensor\n",
        "        else:\n",
        "            self.dtype = torch.FloatTensor\n",
        "            \n",
        "        self.style_path = style_path\n",
        "        self.result_dir = result_dir\n",
        "        self.weight_dir = weight_dir\n",
        "        self.process_dir = process_dir\n",
        "        self.process_image = process_image\n",
        "        self.process_number = process_number\n",
        "        self.process_scale = process_scale\n",
        "        self.record_number = record_number\n",
        "        self.record_name = record_name\n",
        "        self.test_number = test_number\n",
        "        self.test_dir = test_dir\n",
        "        self.transNet = transNet\n",
        "        self.norm_type = norm_type\n",
        "        \n",
        "        # Models\n",
        "        if self.transNet == 'capsnet':\n",
        "            self.trans_net = ImageTransformCapsNet(norm_type=norm_type).type(self.dtype)\n",
        "            self.router = MultiStepRI(self.trans_net, milestones=[3], addition=3, verbose=True)\n",
        "        else:\n",
        "            self.trans_net = ImageTransformNet(norm_type=norm_type).type(self.dtype)\n",
        "        \n",
        "        if pretrain is not None:\n",
        "            self.trans_net.load_state_dict(torch.load(pretrain))\n",
        "        \n",
        "        if lossNet == 'senet50':\n",
        "            self.lossnet = SE_Resnet50().type(self.dtype)\n",
        "            self.size = 224\n",
        "        else:\n",
        "            self.lossnet = VGG16().type(self.dtype)\n",
        "            self.size = 256\n",
        "        \n",
        "        # Dataloader\n",
        "        self.dloader, total_num = data_loader(root=trn_dir, batch_size = batch_size, size = self.size)\n",
        "        self.total_iter = int(total_num / batch_size) + 1\n",
        "        if self.test_dir is not None:\n",
        "            self.test_dloader, test_total_num = data_loader(root=test_dir, batch_size = batch_size, size = self.size)\n",
        "            self.test_total_iter = int(test_total_num / batch_size) + 1\n",
        "        \n",
        "        # Loss function and optimizer\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        if opti == 'adam':\n",
        "            self.optimizer = optim.Adam(self.trans_net.parameters(), lr=lr, weight_decay=1e-5)\n",
        "        elif opti == 'sgd':\n",
        "            self.optimizer = optim.SGD(self.trans_net.parameters(), lr=lr, weight_decay=1e-5)\n",
        "        else:\n",
        "            self.optimizer = AdamW(self.trans_net.parameters(), lr=lr, weight_decay=1e-5)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.content_loss_pos = content_loss_pos\n",
        "        self.lambda_c = lambda_c\n",
        "        self.lambda_s = lambda_s\n",
        "        self.show_every = show_every\n",
        "        self.save_every = save_every\n",
        "        self.num_epoch = num_epoch\n",
        "        \n",
        "    def train(self):\n",
        "        # Process on style image. Only need to be done once.\n",
        "        style_img = load_img(self.style_path, size=(self.size, self.size)).type(self.dtype)\n",
        "        _style_img = style_img.clone()\n",
        "        style_img = Variable(style_img)\n",
        "        style_img = norm(style_img)\n",
        "\n",
        "        style_relu = self.lossnet(style_img)\n",
        "        if gram=='gramP':\n",
        "            gram_target = gramP(style_relu)\n",
        "        else:\n",
        "            gram_target = gram(style_relu)\n",
        "        \n",
        "        # Write records\n",
        "        count = 0\n",
        "        train_record = 'iter,train_content_loss,train_style_loss\\n'\n",
        "        test_record = 'iter,test_content_loss,test_style_loss\\n'\n",
        "        test_interval = int(self.num_epoch * len(self.dloader) / self.test_number if self.test_number > 0 else -1)\n",
        "        process_interval = int(self.num_epoch * len(self.dloader) / self.process_number if self.process_number > 0 else -1)\n",
        "        record_interval = int(self.num_epoch * len(self.dloader) / self.record_number if self.record_number > 0 else -1)\n",
        "        print('Start traing [test_interval %d, process_interval %d, record_interval %d]' % (test_interval, process_interval, record_interval))\n",
        "\n",
        "        for epoch in range(self.num_epoch):\n",
        "            for iters, (trn_img, _) in enumerate(self.dloader):\n",
        "                # Forward training images to ImageTransformNet\n",
        "                trn_img = Variable(trn_img.type(self.dtype))\n",
        "                trn_img = norm(trn_img)\n",
        "                out_img = self.trans_net(trn_img)\n",
        "                \n",
        "                content_img = Variable(trn_img.data.clone())\n",
        "                out_img = norm(out_img)\n",
        "\n",
        "                # Forward training img and content img to VGG16\n",
        "                relu_target = self.lossnet(content_img)\n",
        "                relu_out = self.lossnet(out_img)\n",
        "\n",
        "                # Get 4 activations from lossNet\n",
        "                feature_y = relu_out[self.content_loss_pos]\n",
        "                feature_t = Variable(relu_target[self.content_loss_pos].data, requires_grad=False)\n",
        "                \n",
        "                # Content loss\n",
        "                content_loss = self.lambda_c * self.mse_loss(feature_y, feature_t)\n",
        "\n",
        "                # Gram matrix\n",
        "                if gram=='gramP':\n",
        "                    gram_out = gramP(relu_out)\n",
        "                else:\n",
        "                    gram_out = gram(relu_out)\n",
        "\n",
        "                # Style matrix\n",
        "                style_loss = 0\n",
        "                for i in range(len(gram_target)):\n",
        "                    gram_y = gram_out[i]\n",
        "                    gram_t = Variable(gram_target[i].expand_as(gram_out[i]).data, requires_grad=False)\n",
        "                    style_loss += self.lambda_s * self.mse_loss(gram_y, gram_t)\n",
        "\n",
        "                loss = content_loss + style_loss\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                count += 1\n",
        "\n",
        "                if iters % self.show_every == 0:\n",
        "                    print('[Epoch : (%d / %d), Iters : (%d / %d), Count : %d] => Content : %f, Style : %f' \\\n",
        "                          %(epoch + 1, self.num_epoch, iters, self.total_iter, count, content_loss.item(), style_loss.item()))\n",
        "                    \n",
        "                    _, style_name = os.path.split(self.style_path)\n",
        "                    style_name, _ = os.path.splitext(style_name)\n",
        "                    result_dir = os.path.join(self.result_dir, style_name)\n",
        "                    \n",
        "                    if os.path.exists(result_dir) is not True:\n",
        "                        os.makedirs(result_dir)\n",
        "                    \n",
        "                    file_name = str(epoch) + '_' + str(iters) + '.png'\n",
        "                    file_name = os.path.join(result_dir, file_name)\n",
        "                    \n",
        "                    # Denorm the img to get correct img\n",
        "                    content_img = denorm(content_img)\n",
        "                    out_img = denorm(out_img)\n",
        "                    \n",
        "                    save_img(file_name, content_img.data[0], _style_img[0], out_img.data[0])\n",
        "                    \n",
        "                if count % record_interval == 1:\n",
        "                    train_record += str(count) + ',' + str(content_loss.item()) + ',' + str(style_loss.item()) + '\\n'\n",
        "                    \n",
        "                if count % process_interval == 1:\n",
        "                    _, style_name = os.path.split(self.style_path)\n",
        "                    style_name, _ = os.path.splitext(style_name)\n",
        "                    \n",
        "                    content_img = load_img(path=self.process_image, scale=self.process_scale)\n",
        "                    content_img = Variable(content_img.type(self.dtype))\n",
        "                    content_img = norm(content_img)\n",
        "                    result_img = self.trans_net(content_img)\n",
        "                    \n",
        "                    if os.path.exists(self.process_dir) is not True:\n",
        "                        os.makedirs(self.process_dir)\n",
        "                    torchvision.utils.save_image(result_img.data, os.path.join(self.process_dir, style_name + '_' + str(count) + '.png'), nrow=1)\n",
        "                    \n",
        "                if count % test_interval == 0 and self.test_dir is not None:\n",
        "                    print('[Epoch : (%d / %d), Iters : (%d / %d), Count : %d] => Content : %f, Style : %f' \\\n",
        "                          %(epoch + 1, self.num_epoch, iters, self.total_iter, count, content_loss.item(), style_loss.item()))\n",
        "                    closs = 0\n",
        "                    sloss = 0\n",
        "                    self.trans_net.eval()\n",
        "                    for iters, (test_img, _) in enumerate(self.test_dloader):\n",
        "                        # Forward training images to ImageTransformNet\n",
        "                        test_img = Variable(test_img.type(self.dtype))\n",
        "                        test_img = norm(test_img)\n",
        "                        out_img = self.trans_net(test_img)\n",
        "\n",
        "                        content_img = Variable(test_img.data.clone())\n",
        "                        out_img = norm(out_img)\n",
        "\n",
        "                        # Forward test img and content img\n",
        "                        relu_target = self.lossnet(content_img)\n",
        "                        relu_out = self.lossnet(out_img)\n",
        "\n",
        "                        # Get 4 activations from lossNet\n",
        "                        feature_y = relu_out[self.content_loss_pos]\n",
        "                        feature_t = Variable(relu_target[self.content_loss_pos].data, requires_grad=False)\n",
        "\n",
        "                        # Content loss\n",
        "                        content_loss = self.lambda_c * self.mse_loss(feature_y, feature_t)\n",
        "\n",
        "                        # Gram matrix\n",
        "                        if gram=='gramP':\n",
        "                            gram_out = gramP(relu_out)\n",
        "                        else:\n",
        "                            gram_out = gram(relu_out)\n",
        "\n",
        "                        # Style matrix\n",
        "                        style_loss = 0\n",
        "                        for i in range(len(gram_target)):\n",
        "                            gram_y = gram_out[i]\n",
        "                            gram_t = Variable(gram_target[i].expand_as(gram_out[i]).data, requires_grad=False)\n",
        "                            style_loss += self.lambda_s * self.mse_loss(gram_y, gram_t)\n",
        "\n",
        "                        closs += content_loss.item()\n",
        "                        sloss += style_loss.item()\n",
        "\n",
        "                    if self.transNet == 'capsnet':\n",
        "                        self.router.step()\n",
        "                    self.trans_net.train()\n",
        "                    test_record += str(count) + ',' + str(closs/len(self.test_dloader)) + ',' + str(sloss/len(self.test_dloader)) + '\\n'\n",
        "                    print('[Epoch : (%d / %d) => Test_Content : %f, Test_Style : %f' \\\n",
        "                          %(epoch + 1, self.num_epoch, closs/len(self.test_dloader), sloss/len(self.test_dloader)))\n",
        "                    \n",
        "        weight_name = style_name + '.weight'\n",
        "        weight_path = os.path.join(self.weight_dir, weight_name)\n",
        "        torch.save(self.trans_net.state_dict(), weight_path)\n",
        "        \n",
        "        if self.record_name is not None:\n",
        "            files = open(self.record_name + '_train.txt',\"w\") # \"w\"\n",
        "            files.write(train_record)\n",
        "            files.close()\n",
        "            if self.test_dir is not None:\n",
        "                files = open(self.record_name + '_test.txt',\"w\") # \"w\"\n",
        "                files.write(test_record)\n",
        "                files.close()\n",
        "        \n",
        "def test(weight_path, content_path, output_path, scale=None, transNet='capsnet', norm_type = 'batch'):\n",
        "    if torch.cuda.is_available():\n",
        "        dtype = torch.cuda.FloatTensor\n",
        "    else:\n",
        "        dtype = torch.FloatTensor\n",
        "\n",
        "    print('Loading the model...')\n",
        "    if transNet=='capsnet':\n",
        "        trans_net = ImageTransformCapsNet(norm_type=norm_type).type(dtype)\n",
        "    else:\n",
        "        trans_net = ImageTransformNet(norm_type=norm_type).type(dtype)\n",
        "    trans_net = load_weight(model=trans_net, path=weight_path)\n",
        "\n",
        "    print('Loading the model is done!')\n",
        "    # content_img = (1, 3, 256, 256)\n",
        "    content_img = load_img(path=content_path, scale=scale)\n",
        "    content_img = Variable(content_img.type(dtype))\n",
        "    content_img = norm(content_img)\n",
        "\n",
        "    # result_img = (1, 3, 256, 256)\n",
        "    result_img = trans_net(content_img)\n",
        "\n",
        "    # content_img = (1, 3, 256, 256)\n",
        "    content_img = denorm(content_img)\n",
        "\n",
        "    out_dir, _ = os.path.split(output_path)\n",
        "    if os.path.exists(out_dir) is not True and out_dir != '':\n",
        "        os.makedirs(out_dir)\n",
        "\n",
        "    torchvision.utils.save_image(result_img.data, output_path, nrow=1)\n",
        "    print('Saved image : ' + output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fL7xvcSZRbs8",
        "colab_type": "code",
        "outputId": "69ffe277-9031-41c1-cafa-1e2a3ab1b9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3847
        }
      },
      "cell_type": "code",
      "source": [
        "# train\n",
        "s = Solver(trn_dir = '../Perceptual/pytorch_v/data',\n",
        "           style_path = 'style/abs.jpg', \n",
        "           record_name = 'abstract_1_caps_record',\n",
        "           result_dir = 'check', \n",
        "           weight_dir = './',\n",
        "           num_epoch = 3,\n",
        "           batch_size = 5,\n",
        "           content_loss_pos = 1,\n",
        "           lr = 1e-3,\n",
        "           lambda_c = 1,\n",
        "           lambda_s = 5e4, #5e4 1e6\n",
        "           show_every = 20,\n",
        "           save_every = 5000,\n",
        "           pretrain = None,\n",
        "           lossNet = 'vgg', # vgg senet50， \n",
        "           process_dir = 'process', \n",
        "           process_image = 'content/ybh.jpg', \n",
        "           process_scale = 0.3, \n",
        "           process_number  = 20, \n",
        "           record_number = 600,\n",
        "           test_dir = '../Perceptual/pytorch_v/valid',\n",
        "           test_number = 5,\n",
        "           transNet = 'capsnet', # capsnet cnn\n",
        "           opti = 'adamw', # adam adamw sgd\n",
        "           norm_type = 'instance', # batch instance\n",
        "           gram = 'gram' # gram gramP(Double Gram)\n",
        "          )\n",
        "\n",
        "s.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing pretrained VGG 16 ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.torch/models/vgg16-397923af.pth\n",
            "100%|██████████| 553433881/553433881 [00:05<00:00, 93298593.56it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start traing [test_interval 597, process_interval 149, record_interval 4]\n",
            "[Epoch : (1 / 3), Iters : (0 / 995), Count : 1] => Content : 8.577991, Style : 81.841286\n",
            "[Epoch : (1 / 3), Iters : (20 / 995), Count : 21] => Content : 6.189469, Style : 72.253922\n",
            "[Epoch : (1 / 3), Iters : (40 / 995), Count : 41] => Content : 12.414554, Style : 37.415215\n",
            "[Epoch : (1 / 3), Iters : (60 / 995), Count : 61] => Content : 10.282390, Style : 31.595238\n",
            "[Epoch : (1 / 3), Iters : (80 / 995), Count : 81] => Content : 12.468610, Style : 24.234875\n",
            "[Epoch : (1 / 3), Iters : (100 / 995), Count : 101] => Content : 13.713984, Style : 19.657730\n",
            "[Epoch : (1 / 3), Iters : (120 / 995), Count : 121] => Content : 11.664084, Style : 17.933031\n",
            "[Epoch : (1 / 3), Iters : (140 / 995), Count : 141] => Content : 15.917862, Style : 13.623446\n",
            "[Epoch : (1 / 3), Iters : (160 / 995), Count : 161] => Content : 12.860255, Style : 13.660542\n",
            "[Epoch : (1 / 3), Iters : (180 / 995), Count : 181] => Content : 13.364314, Style : 12.096566\n",
            "[Epoch : (1 / 3), Iters : (200 / 995), Count : 201] => Content : 11.621054, Style : 12.422026\n",
            "[Epoch : (1 / 3), Iters : (220 / 995), Count : 221] => Content : 12.326928, Style : 12.078882\n",
            "[Epoch : (1 / 3), Iters : (240 / 995), Count : 241] => Content : 12.348803, Style : 9.799674\n",
            "[Epoch : (1 / 3), Iters : (260 / 995), Count : 261] => Content : 12.899905, Style : 9.538352\n",
            "[Epoch : (1 / 3), Iters : (280 / 995), Count : 281] => Content : 12.996653, Style : 9.624472\n",
            "[Epoch : (1 / 3), Iters : (300 / 995), Count : 301] => Content : 13.104100, Style : 8.632688\n",
            "[Epoch : (1 / 3), Iters : (320 / 995), Count : 321] => Content : 13.298502, Style : 9.544613\n",
            "[Epoch : (1 / 3), Iters : (340 / 995), Count : 341] => Content : 13.639918, Style : 10.031170\n",
            "[Epoch : (1 / 3), Iters : (360 / 995), Count : 361] => Content : 13.382439, Style : 8.423513\n",
            "[Epoch : (1 / 3), Iters : (380 / 995), Count : 381] => Content : 12.519464, Style : 8.146001\n",
            "[Epoch : (1 / 3), Iters : (400 / 995), Count : 401] => Content : 12.942442, Style : 9.219454\n",
            "[Epoch : (1 / 3), Iters : (420 / 995), Count : 421] => Content : 12.696923, Style : 9.032895\n",
            "[Epoch : (1 / 3), Iters : (440 / 995), Count : 441] => Content : 12.558192, Style : 7.685439\n",
            "[Epoch : (1 / 3), Iters : (460 / 995), Count : 461] => Content : 12.440854, Style : 7.620520\n",
            "[Epoch : (1 / 3), Iters : (480 / 995), Count : 481] => Content : 12.414169, Style : 7.087622\n",
            "[Epoch : (1 / 3), Iters : (500 / 995), Count : 501] => Content : 11.662887, Style : 7.070668\n",
            "[Epoch : (1 / 3), Iters : (520 / 995), Count : 521] => Content : 12.490669, Style : 7.263874\n",
            "[Epoch : (1 / 3), Iters : (540 / 995), Count : 541] => Content : 12.445650, Style : 7.227141\n",
            "[Epoch : (1 / 3), Iters : (560 / 995), Count : 561] => Content : 11.982234, Style : 6.777260\n",
            "[Epoch : (1 / 3), Iters : (580 / 995), Count : 581] => Content : 13.942427, Style : 6.880929\n",
            "[Epoch : (1 / 3), Iters : (596 / 995), Count : 597] => Content : 12.453974, Style : 6.444753\n",
            "[Epoch : (1 / 3) => Test_Content : 12.453394, Test_Style : 6.579956\n",
            "[Epoch : (1 / 3), Iters : (600 / 995), Count : 601] => Content : 12.366587, Style : 7.121481\n",
            "[Epoch : (1 / 3), Iters : (620 / 995), Count : 621] => Content : 11.931432, Style : 6.585699\n",
            "[Epoch : (1 / 3), Iters : (640 / 995), Count : 641] => Content : 12.375151, Style : 6.519219\n",
            "[Epoch : (1 / 3), Iters : (660 / 995), Count : 661] => Content : 12.577561, Style : 6.013904\n",
            "[Epoch : (1 / 3), Iters : (680 / 995), Count : 681] => Content : 11.472835, Style : 6.971075\n",
            "[Epoch : (1 / 3), Iters : (700 / 995), Count : 701] => Content : 11.753271, Style : 6.345781\n",
            "[Epoch : (1 / 3), Iters : (720 / 995), Count : 721] => Content : 11.376348, Style : 6.699483\n",
            "[Epoch : (1 / 3), Iters : (740 / 995), Count : 741] => Content : 12.431358, Style : 6.154101\n",
            "[Epoch : (1 / 3), Iters : (760 / 995), Count : 761] => Content : 11.529325, Style : 6.795716\n",
            "[Epoch : (1 / 3), Iters : (780 / 995), Count : 781] => Content : 12.167074, Style : 5.622558\n",
            "[Epoch : (1 / 3), Iters : (800 / 995), Count : 801] => Content : 11.575884, Style : 5.267140\n",
            "[Epoch : (1 / 3), Iters : (820 / 995), Count : 821] => Content : 12.401423, Style : 5.784199\n",
            "[Epoch : (1 / 3), Iters : (840 / 995), Count : 841] => Content : 11.864403, Style : 5.147573\n",
            "[Epoch : (1 / 3), Iters : (860 / 995), Count : 861] => Content : 11.286444, Style : 5.514700\n",
            "[Epoch : (1 / 3), Iters : (880 / 995), Count : 881] => Content : 11.282992, Style : 6.221213\n",
            "[Epoch : (1 / 3), Iters : (900 / 995), Count : 901] => Content : 10.551825, Style : 6.301306\n",
            "[Epoch : (1 / 3), Iters : (920 / 995), Count : 921] => Content : 11.631982, Style : 5.217287\n",
            "[Epoch : (1 / 3), Iters : (940 / 995), Count : 941] => Content : 12.582949, Style : 5.369839\n",
            "[Epoch : (1 / 3), Iters : (960 / 995), Count : 961] => Content : 11.559191, Style : 4.870706\n",
            "[Epoch : (1 / 3), Iters : (980 / 995), Count : 981] => Content : 12.566941, Style : 4.764526\n",
            "[Epoch : (2 / 3), Iters : (0 / 995), Count : 996] => Content : 12.646312, Style : 4.631325\n",
            "[Epoch : (2 / 3), Iters : (20 / 995), Count : 1016] => Content : 10.985810, Style : 5.193728\n",
            "[Epoch : (2 / 3), Iters : (40 / 995), Count : 1036] => Content : 11.157817, Style : 4.899165\n",
            "[Epoch : (2 / 3), Iters : (60 / 995), Count : 1056] => Content : 11.595471, Style : 4.970348\n",
            "[Epoch : (2 / 3), Iters : (80 / 995), Count : 1076] => Content : 11.391770, Style : 4.213981\n",
            "[Epoch : (2 / 3), Iters : (100 / 995), Count : 1096] => Content : 10.910514, Style : 4.603478\n",
            "[Epoch : (2 / 3), Iters : (120 / 995), Count : 1116] => Content : 11.399493, Style : 4.130429\n",
            "[Epoch : (2 / 3), Iters : (140 / 995), Count : 1136] => Content : 11.098218, Style : 4.756341\n",
            "[Epoch : (2 / 3), Iters : (160 / 995), Count : 1156] => Content : 10.554216, Style : 4.714462\n",
            "[Epoch : (2 / 3), Iters : (180 / 995), Count : 1176] => Content : 11.379904, Style : 4.359844\n",
            "[Epoch : (2 / 3), Iters : (198 / 995), Count : 1194] => Content : 10.607739, Style : 5.357204\n",
            "[Epoch : (2 / 3) => Test_Content : 10.479213, Test_Style : 4.703847\n",
            "[Epoch : (2 / 3), Iters : (200 / 995), Count : 1196] => Content : 11.592635, Style : 4.436172\n",
            "[Epoch : (2 / 3), Iters : (220 / 995), Count : 1216] => Content : 10.905740, Style : 4.394188\n",
            "[Epoch : (2 / 3), Iters : (240 / 995), Count : 1236] => Content : 11.109900, Style : 4.463244\n",
            "[Epoch : (2 / 3), Iters : (260 / 995), Count : 1256] => Content : 11.061237, Style : 4.260819\n",
            "[Epoch : (2 / 3), Iters : (280 / 995), Count : 1276] => Content : 11.104746, Style : 3.978252\n",
            "[Epoch : (2 / 3), Iters : (300 / 995), Count : 1296] => Content : 10.289292, Style : 4.211083\n",
            "[Epoch : (2 / 3), Iters : (320 / 995), Count : 1316] => Content : 10.103914, Style : 5.023044\n",
            "[Epoch : (2 / 3), Iters : (340 / 995), Count : 1336] => Content : 9.929219, Style : 4.527783\n",
            "[Epoch : (2 / 3), Iters : (360 / 995), Count : 1356] => Content : 10.423655, Style : 3.966769\n",
            "[Epoch : (2 / 3), Iters : (380 / 995), Count : 1376] => Content : 9.860498, Style : 4.253824\n",
            "[Epoch : (2 / 3), Iters : (400 / 995), Count : 1396] => Content : 10.112635, Style : 4.213417\n",
            "[Epoch : (2 / 3), Iters : (420 / 995), Count : 1416] => Content : 9.982450, Style : 4.296704\n",
            "[Epoch : (2 / 3), Iters : (440 / 995), Count : 1436] => Content : 10.682344, Style : 4.726390\n",
            "[Epoch : (2 / 3), Iters : (460 / 995), Count : 1456] => Content : 9.872321, Style : 4.678342\n",
            "[Epoch : (2 / 3), Iters : (480 / 995), Count : 1476] => Content : 10.579769, Style : 4.365531\n",
            "[Epoch : (2 / 3), Iters : (500 / 995), Count : 1496] => Content : 10.253831, Style : 4.112064\n",
            "[Epoch : (2 / 3), Iters : (520 / 995), Count : 1516] => Content : 9.683537, Style : 4.046280\n",
            "[Epoch : (2 / 3), Iters : (540 / 995), Count : 1536] => Content : 9.673094, Style : 4.312936\n",
            "[Epoch : (2 / 3), Iters : (560 / 995), Count : 1556] => Content : 10.565310, Style : 3.728595\n",
            "[Epoch : (2 / 3), Iters : (580 / 995), Count : 1576] => Content : 9.944887, Style : 5.060869\n",
            "[Epoch : (2 / 3), Iters : (600 / 995), Count : 1596] => Content : 9.718262, Style : 3.970370\n",
            "[Epoch : (2 / 3), Iters : (620 / 995), Count : 1616] => Content : 9.109126, Style : 4.128632\n",
            "[Epoch : (2 / 3), Iters : (640 / 995), Count : 1636] => Content : 9.585861, Style : 3.470500\n",
            "[Epoch : (2 / 3), Iters : (660 / 995), Count : 1656] => Content : 9.473801, Style : 3.579815\n",
            "[Epoch : (2 / 3), Iters : (680 / 995), Count : 1676] => Content : 9.810852, Style : 3.537424\n",
            "[Epoch : (2 / 3), Iters : (700 / 995), Count : 1696] => Content : 9.922663, Style : 4.052342\n",
            "[Epoch : (2 / 3), Iters : (720 / 995), Count : 1716] => Content : 8.620479, Style : 3.381266\n",
            "[Epoch : (2 / 3), Iters : (740 / 995), Count : 1736] => Content : 9.534578, Style : 4.454851\n",
            "[Epoch : (2 / 3), Iters : (760 / 995), Count : 1756] => Content : 8.741627, Style : 4.076557\n",
            "[Epoch : (2 / 3), Iters : (780 / 995), Count : 1776] => Content : 9.035811, Style : 3.576218\n",
            "[Epoch : (2 / 3), Iters : (795 / 995), Count : 1791] => Content : 9.381147, Style : 3.754153\n",
            "Epoch 3: increasing module upsamples.0.conv_trans_block.0' routing iterations to 6.\n",
            "Epoch 3: increasing module upsamples.1.conv_trans_block.0' routing iterations to 6.\n",
            "[Epoch : (2 / 3) => Test_Content : 9.244798, Test_Style : 3.779274\n",
            "[Epoch : (2 / 3), Iters : (800 / 995), Count : 1796] => Content : 9.376284, Style : 4.174970\n",
            "[Epoch : (2 / 3), Iters : (820 / 995), Count : 1816] => Content : 9.328574, Style : 3.493737\n",
            "[Epoch : (2 / 3), Iters : (840 / 995), Count : 1836] => Content : 8.716902, Style : 3.534491\n",
            "[Epoch : (2 / 3), Iters : (860 / 995), Count : 1856] => Content : 8.908621, Style : 3.683341\n",
            "[Epoch : (2 / 3), Iters : (880 / 995), Count : 1876] => Content : 9.212460, Style : 3.913342\n",
            "[Epoch : (2 / 3), Iters : (900 / 995), Count : 1896] => Content : 8.806199, Style : 3.620361\n",
            "[Epoch : (2 / 3), Iters : (920 / 995), Count : 1916] => Content : 8.541113, Style : 3.710753\n",
            "[Epoch : (2 / 3), Iters : (940 / 995), Count : 1936] => Content : 8.852934, Style : 3.967998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2675c9fc3cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m           )\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-80e0be60d3bd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m                     \u001b[0mcontent_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                     \u001b[0mcontent_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                     \u001b[0mresult_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-80e0be60d3bd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-80e0be60d3bd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_trans_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/capsule_layer/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    278\u001b[0m         return CL.capsule_conv_transpose2d(input, self.weight, self.stride, self.padding, self.output_padding,\n\u001b[1;32m    279\u001b[0m                                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouting_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                                            self.bias, self.training, **self.kwargs)\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/capsule_layer/functional.py\u001b[0m in \u001b[0;36mcapsule_conv_transpose2d\u001b[0;34m(input, weight, stride, padding, output_padding, dilation, routing_type, num_iterations, dropout, bias, training, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdynamic_routing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mrouting_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'k_means'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_means_routing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} routing algorithm is not implemented.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouting_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/capsule_layer/functional.py\u001b[0m in \u001b[0;36mk_means_routing\u001b[0;34m(input, bias, num_iterations, similarity, squash, return_prob, softmax_dim)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cosine'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 392.50 MiB (GPU 0; 11.17 GiB total capacity; 9.04 GiB already allocated; 105.44 MiB free; 1.71 GiB cached)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "WKEjHHG5Jjq7",
        "colab_type": "code",
        "outputId": "b4bddbf9-c4fd-415f-a044-32b0519f22d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# test\n",
        "content_name = 'tp.jpg'\n",
        "test(\n",
        "    weight_path='new_weight/udnie.weight' ,\n",
        "    content_path='content/' + content_name, \n",
        "    output_path='fantasy_' + content_name.split('.')[0] + '.png',\n",
        "    scale=0.9,\n",
        "    transNet='capsnet',\n",
        "    norm_type='instance', # batch instance\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the model...\n",
            "Loading the model is done!\n",
            "Saved image : fantasy_tp.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "89L0Bwqh9cfc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# draw network\n",
        "print(SE_Resnet50(),'\\n\\n',VGG16(),'\\n\\n',ImageTransformCapsNet(norm_type='instance'),'\\n\\n',ImageTransformNet(norm_type='instance'))\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "x=torch.ones((1,3,224,224))\n",
        "model=ImageTransformCapsNet(norm_type='instance')\n",
        "make_dot(model(x), params=dict(list(model.named_parameters())))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}